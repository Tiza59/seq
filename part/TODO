Running n=5 with NOPREP found 3.6E6 partitions with max size 2 in an hour.
We then coredumped after 7.5h having found a further 3E7 partitions with
max size 3, probably due to running over the available address space -
storing 3.36E6 AVL tree elements costs us 1.5GB (16 bytes overhead per
element, plus 32 bytes of data).

I can't tell for sure because the coredump was truncated, but I'm guessing
we tried to grow one of the trees by 50%, and blew the 2GB limit.

We simply can't afford to de-dupe by recording seen partials, so we must
instead de-dupe by canon - we must select a canonicalization process that
ensures each partial appears at least once in already-canonical form.

[[[
This is partially working now, but we need to further refine the definition
of what is canonical - ensuring "each partial appears at least once in
already-canonical form" requires a canonicalization that guarantees to be
progressive, ie that guarantees every canonical partial of C<k+1> pieces
has a canonical predecessor of C<k> pieces.

Failing to do so means the current code, while finding all solutions for
n <= 3, misses about 10% of solutions for n=4.
]]]

After that, next step is to add code to convert a vec_t into an array of
graph_t, to find the number of ways to partition a graph_t, and to use
that wherever we can safely do so.

Symmetries are the problem: given an asymmetrical large piece filling
more than half the shape, it is sufficient to count the partitions of
each of the disconnected subgraphs comprising the remained and multiply
them together. But if it's a symmetrical piece, or multiple pieces, or
no more than half the shape, there are a bunch of complex wrinkles to
take into consideration.

